Dies ist die Beschreibung des Moduls "objectWebCrawler" mit dem Namen "Objekt Web Crawler": Der Objekt Web Crawler durchläuft die Datenbank und legt zu den gefunden Objekten automatisch Datenkonnektoren an. Die Datenkonnektoren erhalten dann passend zu den Objekten Datenfelder, so dass am Ende die Datenfelder eines Datenkonnektors die Obermenge der verwendeten Slots und Member aller Objekte dieses Datenkonnektors repräsentieren. Eine genaue Beschreibung der Logik des Crawlers befindet sich in der Technischen Dokumentation. Dies ist die Beschreibung der Funktionalität des Moduls "objectWebCrawler" mit dem Namen "Objekt Web Crawler" bezüglich Eingabefenster: Dieses Fenster dient der Einstellungen und dem Starten des Crawlers. Dies ist die Beschreibung der Funktionalität des Moduls "objectWebCrawler" mit dem Namen "Objekt Web Crawler" bezüglich Implementation: Der Objekt Web Crawler durchsucht die Datenbank nach Objekten und erstellt Datenkonnektoren zu den gefundenen Objekten und verbucht die Slots und Member der Objekte als Datenfelder in den Datenkonnektoren. Der Crawler startet dabei immer bei den Objekten der Root-Entry-Point Collections (REP) und verfolgt von diesen Objekten ausgehend alle Verweise auf andere Objekte. Der Crawler beginnt mit den Objekten, die sich beim Start in der Warteschlange befunden haben. Alle Objekte, die von diesen aus gefunden werden, werden der Warteschlange hinzugefügt und diese Objekt für Objekt abgearbeitet. Sobald die Warteschlange leer ist, werden die Objekte aller REPs mit dem REP-Startindex der Warteschlange hinzugefügt und die Abarbeitung der Warteschlange beginnt erneut. Nachdem die Warteschlange wieder abgearbeitet ist, wird der REP-Startindex imkrementiert und der Prozess beginnt erneut, bis der höchste Index der größten REP erreicht wurde. Da immer erst alle Objekte zu einem REP-Startindex abgearbeitet sein müssen und bereits verarbeitete Objekte ignoriert werden, tritt der Effekt auf, dass die Verarbeitung des ersten REP-Index sehr lange dauert und die restlichen Indizies dann sehr schnell abgearbeitet werden. Dieser Effekt kann durch Abwählen der Option REP-Objekte in Warteschlange deutlich abgemildert werden. Der grundlegende Ablauf des Crawlers ist damit skizziert. Allerdings gibt es einige zusätzliche Regeln und Sonderfälle, die in den folgenden Unterabschnitten gesondert betrachtet werden. Dies ist die Beschreibung der Funktionalität des Moduls "objectWebCrawler" mit dem Namen "Objekt Web Crawler" bezüglich Haupt- und Hilfsdatenkonnektoren: Für Berichte und Auswertungen sind nicht alle Objekte gleichermaßen interessant. Es gibt bestimmte Objekte, nach denen man suchen kann und über die man Auswertungen fahren würde, sogenannte Hauptobjekte. Andere Objekte werden nur über diese Hauptobjekte erreicht und nie gesondert betrachtet. Das hauptsächliche Kriterium für ein Hauptobjekt ist, dass es in einer REP ist. Außerdem sind Objekte der Klassen auf der Whitelist auch Hauptobjekte. Objekte auf der Blacklist sind natürlich selbst dann keine Hauptobjekte wenn sie in einer REP sind. Für Hauptobjekte werden Datenkonnektoren in der REP der Datenkonnektoren angelegt. Diese Datenkonnektoren nennen sich dann auch Hauptdatenkonnektoren. Für Hilfsobjekte werden Datenkonnektoren hierarchisch unterhalb der Hauptdatenkonnektoren angelegt, die dann nicht in der REP der Datenkonnektoren gespeichert werden. Diese Hilfsdatenkonnektoren sind also nur über die Hauptdatenkonnektoren erreichbar, genau wie die entsprechenden Objekte auch nur über die Hauptobjekte erreicht werden können. Es ist möglich, dass eine bestimmte Klasse sowohl als Haupt- als auch als Hilfsdatenkonnektor auftaucht. Dieser Zustand tritt ein wenn die Objekte der Klasse standardmäßig in der REP gespeichert werden, einzelne Objekte jedoch bewusst nicht in der REP registriert werden. Dies ist die Beschreibung der Funktionalität des Moduls "objectWebCrawler" mit dem Namen "Objekt Web Crawler" bezüglich Black- und Whitelist: Der Algorithmus zum Bestimmen der Haupt- und Hilfsdatenkonnektoren braucht für manche Typen eine Feinjustierung. Auf der einen Seite gibt es Klassen für die überhaupt kein Datenkonnektor angelegt werden soll, auf der anderen Seite sollen manche Objekte als Hauptobjekte betrachtet werden, obwohl sie in keiner REP sind. Für diese Anpassungen gibt es die Blacklist und die Whitelist. Auf der Black- und Whitelist stehen Klassen. Alle Objekte, die zu diesen Klassen oder zu von diesen abgeleiteten Klassen gehören, gelten als zugehörig zur entsprechenden Liste. Die Blacklist beschreibt Objekte, für die kein Datenkonnektor angelegt werden soll. Wenn eine Referenz auf ein solches Objekt als Datenfeld verbucht wird, so wird nur der referenzierte Typ gesetzt, jedoch kein Datenkonnektor angelegt. Auf der Blacklist stehen folgende Klassen: Die Whitelist beschreibt Objekte, für die auch dann ein Hauptdatenkonnektor angelegt werden soll, wenn sie in keiner REP sind. Auf der Whitelist stehen folgende Klassen:. Dies ist die Beschreibung der Funktionalität des Moduls "objectWebCrawler" mit dem Namen "Objekt Web Crawler" bezüglich Finden des richtigen Hauptobjektes: Aufgrund der Arbeitsweise des Crawlers, dass er von den Objekten der REPs aus einfach alle Verweise zu anderen Objekten durchläuft, ist der Pfad, den er zu einem Objekt genommen hat nicht unbedingt der kürzeste oder semantisch richtige. Daher müssen zu jedem Hilfsobjekt das passende Hauptobjekt und der Pfad zwischen beiden gesucht werden. Zu diesem Zweck erhalten alle Slots und Member Kosten zugewiesen. Die Kosten sind so gestaltet, dass sie geringer sind, wenn der Pfad potentiell semantisch im Modell aufsteigend ist. Die Kosten werden in der folgenden Tabelle aufgeführt. Zum einen werden die Kosten für den vom Crawler genommenen Pfad und zum anderen die Kosten zu allen erreichbaren Hauptobjekten berechnet. Der Pfad mit den geringsten Kosten und das zugehörige Hauptobjekt werden dann als richtig angenommen. Dies ist die Beschreibung der Funktionalität des Moduls "objectWebCrawler" mit dem Namen "Objekt Web Crawler" bezüglich Wrapper: Für Wrapper selbst werden keine Datenkonnektoren angelegt. Stattdessen werden Datenfelder so angelegt als würde direkt auf die gewrappten Objekte verwiesen werden. Zusätzlich werden für die Member und Slots der Wrapper Datenfelder mit dem Präfix des eigentlichen Member oder Slots angelegt. So kann also zum Beispiel später gesehen werden, welche Slots von einem Wrapper hinzugefügt oder überschrieben wurden. Dies ist die Beschreibung der Funktionalität des Moduls "objectWebCrawler" mit dem Namen "Objekt Web Crawler" bezüglich Attribut-Tabellen: Für Attribut-Tabellen wird kein Datenkonnektor angelegt. Stattdessen werden die Attribute, die in der Tabelle enthalten sind, mit dem Präfix des Member oder Slots direkt als Datenfelder verbucht. So kann man direkt im Datenkonnektor sehen, welche Attribute in der referenzierten Tabelle enthalten sind. Dies ist die Beschreibung der Funktionalität des Moduls "objectWebCrawler" mit dem Namen "Objekt Web Crawler" bezüglich Konten: Für Konten, die in einer REP vorkommen, wird ganz normal ein Hauptdatenkonnektor erstellt. Für alle anderen Konten wird kein Datenkonnektor angelegt. Stattdessen werden die Member und Slots des Kontos als Datenfelder mit dem Präfix des verweisenden Members oder Slots direkt im Datenkonnektor verbucht. Dies ist die Beschreibung der Funktionalität des Moduls "objectWebCrawler" mit dem Namen "Objekt Web Crawler" bezüglich Nachbearbeitung: Sobald der eigentliche Crawling-Vorgang abgeschlossen ist, findet noch eine Nachbearbeitung der Datenkonnektoren statt. Diese Nachbearbeitung kann über das Menü auch manuell aufgerufen werden und beinhaltet die in den folgenden Unterabschnitten angegebenen Schritte. Dies ist die Beschreibung der Funktionalität des Moduls "objectWebCrawler" mit dem Namen "Objekt Web Crawler" bezüglich Anlegen von Teilzugriffspfaden für lange Zugriffspfade: Wenn ein Zugriffspfad mehrere Schritte enthält, so wird für jeden dieser Schritte ein eigenes Datenfeld angelegt sofern es nicht schon vorhanden ist. Dabei wird versucht den Typ des Datenfeldes, Typ des referenzierten Objektes und den Namen über die Informationen aus DDI oder Slots automatisch zu ermitteln. Sollten diese Informationen nicht existieren, wird der Zugriffspfad auf den Objekten des Datenkonnektors ausgewertet und versucht die Information darüber zu ermitteln. Außerdem werden referenzierte Hauptdatenkonnektoren automatisch mit dem neuen Datenfeld verknüpft. Sollte bisher kein Name für das Datenfeld gefunden sein, wird dann der Name des Datenkonnektors für das Datenfeld verwendet. Dies ist die Beschreibung der Funktionalität des Moduls "objectWebCrawler" mit dem Namen "Objekt Web Crawler" bezüglich Berechnung des Abstandes eines Datenfeldes: Zu jedem Datenfeld wird die Länge des Zugriffsausdrucks im Slot associated.counter gespeichert. Diese Länge beschreibt die Anzahl an Schritten im Zugriffsausdruck. Dies ist die Beschreibung der Funktionalität des Moduls "objectWebCrawler" mit dem Namen "Objekt Web Crawler" bezüglich Anlegen eines Datenfeldes CX_KLASSE::this für jeden Datenkonnektor: Für jeden Datenkonnektor wird ein Datenfeld mit dem Zugriffsausdruck CX_KLASSE::this angelegt, das auf den Datenkonnektor selbst verweist. Dies ist die Beschreibung der Funktionalität des Moduls "objectWebCrawler" mit dem Namen "Objekt Web Crawler" bezüglich Performance-Optimierung: Jede Datenbank und jede Anforderung an den Crawler ist anders, daher lässt sich kein eindeutig bester Satz an Parametern für alle Datenbanken angeben. An dieser Stelle sollen jedoch die Parameter beschrieben werden, die sich hauptsächlich auf die Performance auswirken. Die Parameter führen meist dazu, dass weniger Objekte untersucht werden, die jedoch trotzdem das Gesamtbild abbilden sollen. Es sollte jedoch klar sein, dass durch eine Verringerung der untersuchten Objekte potentiell das Crawlingergebnis ungenauer wird. Dies ist die Beschreibung der Funktionalität des Moduls "objectWebCrawler" mit dem Namen "Objekt Web Crawler" bezüglich Rekursionstiefe: Die Rekursionstiefe gibt an wie weit ein Objekt von einem REP-Objekt entfernt ist, also wie viele Verweise abgelaufen werden müssen, um zu diesem Objekt zu kommen. Sollten es mehr Schritte als die Rekursionstiefe sein, wird das Objekt nicht verarbeitet. Eine hohe Rekursionstiefe sorgt für stark verschachtelte Datenkonnektoren. Wenn viel mit verschachtelten Daten gearbeitet wird, zum Beispiel Unterpositionen von Unterpositionen in Aufträgen, muss die Rekursionstiefe höher gewählt werden. Ein Wert von -1 deaktiviert die Prüfung der Rekursionstiefe. Dies ist die Beschreibung der Funktionalität des Moduls "objectWebCrawler" mit dem Namen "Objekt Web Crawler" bezüglich Maximale Collection-Tiefe: Die maximale Collection-Tiefe gibt an wie viele Objekte einer Collection maximal untersucht werden sollen. Wenn immer gleichartige Objekte in einer Collection stehen, reduziert eine hohe Collection-Tiefe die Performance drastisch ohne einen Erkenntnisgewinn. Sind jedoch immer unterschiedliche Objekte in Collections vorhanden und sie werden nicht alle untersucht, werden manche Ausprägungen nicht als Datenkonnektoren verbucht. Hier gilt also dass der Wert umso niedriger gewählt werden kann und sollte je ähnlicher sich Objekte innerhalb derselben Collection sind. Ein Wert von 0 sorgt dafür, dass immer alle Objekte einer Collection betrachtet werden. Dies ist die Beschreibung der Funktionalität des Moduls "objectWebCrawler" mit dem Namen "Objekt Web Crawler" bezüglich Transaktionssplitting: Das Transaktionssplitting gibt an nach wie vielen untersuchten Objekten die aktuelle Transaktion abgeschlossen und eine neue gestartet werden soll. Das Abschließen der Transaktion sorgt auf der einen Seite dafür, dass Zwischenstände dauerhaft in der Datenbank gespeichert werden und verhindert auf der anderen Seite den Effekt, dass mit zu vielen Änderungen innerhalb einer Transaktion das System langsamer wird. Allerdings bremst ein zu häufiges Abschließen von Transaktionen das System auch aus. Die Statistik wird immer mit dem Abschließen der Transaktion aktualisiert. Dies ist die Beschreibung der Funktionalität des Moduls "objectWebCrawler" mit dem Namen "Objekt Web Crawler" bezüglich REP-Objekte in Warteschlange: Mit dieser Option wird gesteuert, ob REP-Objekte, die während des Crawlens erreicht werden in die Warteschlange eingefügt werden sollen oder nicht. Wenn REP-Objekte in die Warteschlange eingefügt werden entsteht der Effekt, dass von den Objekten des ersten REP-Index schon fast alle Objekte erreicht werden und die Abarbeitung des ersten Index sehr lange dauert und die anderen Indizies dann schnell durchlaufen werden können, weil die meisten Objekte schon analysiert wurden. Das sorgt aber auch dafür, dass die Warteschlange sehr groß und somit auch langsam wird. Ist die Option abgewählt, verteilt sich die Arbeit besser auf die REP-Indizies und die Warteschlange wächst nicht groß an. Wird sowieso die ganze Datenbank durchlaufen, wirkt sich das An- oder Abwählen der Option nicht auf das Ergebnis aus, da eh alle Objekte der REPs durchlaufen werden. Wird jedoch nur ein einzelner REP-Index durchlaufen oder nur einzelne Objekte untersucht, würde das Abwählen der Option bedeuten, dass erreichte REP-Objekte nicht untersucht werden. Die Empfehlung ist daher die Option abzuwählen wenn die ganze Datenbank durchsucht wird und sie auszuwählen wenn nur ein Teil der Datenbank untersucht werden soll.