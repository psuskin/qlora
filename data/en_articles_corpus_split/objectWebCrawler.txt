objectWebCrawler: The object web crawler runs through the database and automatically creates data connectors for the objects found. The data connectors then receive data fields matching the objects, so that in the end the data fields of a data connector represent the superset of the used slots and members of all objects of this data connector. A detailed description of the crawler's logic can be found in the technical documentation. Input window: This window is used to configure and start the crawler. Implementation: The object web crawler searches the database for objects and creates data connectors to the found objects and books the slots and members of the objects as data fields in the data connectors. The crawler always starts with the objects of the Root-Entry-Point Collections (REP) and follows all references to other objects starting from these objects. The crawler starts with the objects that were in the queue when it was started. All objects found from them are added to the queue and processed object by object. As soon as the queue is empty, the objects of all REPs with the REP start index are added to the queue and the processing of the queue starts again. After the queue is processed again, the REP start index is incremented and the process starts again until the highest index of the largest REP is reached. Since all objects for a REP start index must always be processed first and already processed objects are ignored, the effect occurs that the processing of the first REP index takes a very long time and the remaining indexes are then processed very quickly. This effect can be significantly mitigated by deselecting the option REP objects in queue. The basic procedure of the crawler is thus outlined. However, there are some additional rules and special cases, which will be considered separately in the following subsections. Main and auxiliary data connectors: Not all objects are equally interesting for reports and evaluations. There are certain objects that you can search for and over which you would run evaluations, so-called main objects. Other objects can only be reached via these main objects and are never considered separately. The main criterion for a main object is that it is in a REP. Furthermore, objects of the classes on the whitelist are also main objects. Objects on the blacklist are of course not main objects even if they are in a REP. For main objects data connectors are created in the REP of the data connectors. These data connectors are then also called main data connectors. For auxiliary objects, data connectors are created hierarchically below the main data connectors, which are then not stored in the REP of the data connectors. These auxiliary data connectors are therefore only accessible via the main data connectors, just as the corresponding objects can only be accessed via the main objects. It is possible that a certain class appears both as main and auxiliary data connector. This state occurs when the objects of the class are stored in the REP by default, but individual objects are deliberately not registered in the REP. Black- and Whitelist: The algorithm for determining the main and auxiliary data connectors needs fine tuning for some types. On the one hand there are classes for which no data connector at all should be created, on the other hand some objects should be considered as main objects although they are not in any REP. For these adjustments there are the blacklist and the whitelist. On the black- and whitelist there are classes. All objects that belong to these classes or to classes derived from them are considered to belong to the corresponding list. The blacklist describes objects for which no data connector is to be created. If a reference to such an object is posted as a data field, only the referenced type is set, but no data connector is created. The blacklist contains the following classes: The whitelist describes objects for which a main data connector should be created even if they are not in any REP. The whitelist contains the following classes:. Finding the right main object: Due to the way the crawler works, that it simply runs through all links to other objects from the objects of the REPs, the path it has taken to an object is not necessarily the shortest or semantically correct one. Therefore, for each auxiliary object the matching main object and the path between both must be searched for. For this purpose, all slots and members are assigned costs. The costs are designed to be lower if the path is potentially semantically ascending in the model. The costs are listed in the following table. On the one hand the costs for the path taken by the crawler and on the other hand the costs to all accessible main objects are calculated. The path with the lowest costs and the corresponding main object are then assumed to be correct. Wrapper: No data connectors are created for wrappers themselves. Instead, data fields are created as if they were directly referenced to the wrapped objects. In addition, data fields are created for the members and slots of the wrapper with the prefix of the actual member or slot. So you can see later which slots have been added or overwritten by a wrapper. Attribute Tables: No data connector is created for attribute tables. Instead, the attributes contained in the table are posted directly as data fields with the prefix of the member or slot. Thus, one can see directly in the data connector which attributes are contained in the referenced table. Accounts: For accounts that appear in a REP, a main data connector is normally created. For all other accounts no data connector is created. Instead, the members and slots of the account are posted directly in the data connector as data fields with the prefix of the referring member or slot. Post-processing: As soon as the actual crawling process has been completed, the data connectors are post-processed. This post-processing can also be called up manually via the menu and includes the steps specified in the following subsections. Creating partial access paths for long access paths: If an access path contains several steps, a separate data field is created for each of these steps if it does not already exist. It is tried to determine the type of the data field, the type of the referenced object and the name automatically using the information from DDI or slots. If this information does not exist, the access path to the objects of the data connector is evaluated and an attempt is made to determine the information about them. Furthermore, referenced main data connectors are automatically linked to the new data field. If no name has been found for the data field so far, the name of the data connector is used for the data field. Calculation of the distance of a data field: For each data field, the length of the access expression is stored in the associated.counter slot. This length describes the number of steps in the access expression. Create a data field CX_KLASSE::this for each data connector: For each data connector a data field is created with the access expression CX_KLASSE::this, which refers to the data connector itself. Performance Optimization: Each database and each requirement for the crawler is different, so it is not possible to specify a unique best set of parameters for all databases. At this point, however, we will describe the parameters that mainly affect performance. The parameters usually result in fewer objects being examined, but they should still reflect the overall picture. However, it should be clear that a reduction in the number of objects examined will potentially make the crawling result less accurate. Recursion depth: The recursion depth indicates how far away an object is from a REP object, i.e. how many references must be run to get to this object. If there are more steps than the recursion depth, the object is not processed. A high recursion depth results in heavily nested data connectors. If a lot of nested data is used, for example subitems of subitems in orders, the recursion depth must be set higher. A value of -1 disables the recursion depth check. Maximum collection depth: The maximum collection depth specifies the maximum number of objects in a collection to be examined. If there are always similar objects in a collection, a high collection depth reduces the performance drastically without a gain in knowledge. However, if there are always different objects in collections and they are not all examined, some characteristics are not posted as data connectors. In this case the value can and should be set lower the more similar objects within the same collection are. A value of 0 ensures that all objects in a collection are always considered. Transaction splitting: The transaction splitting specifies after how many examined objects the current transaction should be completed and a new one started. The completion of the transaction ensures on the one hand that intermediate states are permanently stored in the database and on the other hand prevents the effect that the system slows down with too many changes within a transaction. However, closing transactions too often also slows down the system. The statistics are always updated when the transaction is completed. REP objects in queue: This option controls whether REP objects reached during crawling should be queued or not. If REP objects are inserted into the queue, the effect is that almost all objects of the first REP index are reached and the processing of the first index takes a long time and the other indexes can be processed quickly because most of the objects have already been analysed. But this also causes the queue to become very large and therefore slow. If the option is deselected, the work is better distributed over the REP indices and the queue does not grow very large. If the whole database is run through anyway, selecting or deselecting the option does not affect the result, since all objects of the REPs are run through anyway. However, if only a single REP index is run through or only single objects are examined, unchecking the option would mean that reached REP objects are not examined. The recommendation is therefore to deselect the option if the whole database is searched and to select it if only a part of the database is to be searched. Module name: objectWebCrawler.app. Classes: CX_DATA_CONNECTOR, CX_DATA_FIELD.