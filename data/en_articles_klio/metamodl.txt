This is the description of the module "metamodl" with the name "English:meta model": Due to its business model classes and data fields, Classix does not have a rigid model and therefore no central administration of entities and data fields. In order to be able to describe the dynamically created objects, their attributes and relationships, an ObjectCrawler is implemented with this module, which independently scans the database, collects, examines and categorises objects and stores this information as meta-description. These meta descriptions of the objects and their attributes and relationships can then be maintained here. Before starting the search, you must define the settings for the search, on which the speed and duration of the search depend. The following settings are available: Under the boundary conditions you can specify the maximum age of the meta-info. To speed up the search, meta-information is not overwritten and skipped if it is younger than the set date. Transaction splitting is used to specify after how many transactions (analyses of meta objects) the results are saved in the database. The maximum collection depth specifies how many elements of an M1, MN relation, or collection are analysed (if set to 0, all elements of a collection are searched). The maximum collection depth can significantly influence the duration of the crawling process, a value of 500, and 5 for very large databases, can lead to fast results. Equally important for the duration of the crawling process is the recursion depth parameter (-1 stands for infinite depth). The recursion depth determines how far the crawler should recursively examine the references of an object. Regardless of this setting, the ObjectCrawler always performs a width search, i.e. referenced objects are only examined after all siblings of an object have been examined. The last and most important option is the REP start index, which tells where the object crawler is located when searching the database. The crawler searches the database in such a way that it takes a single object from each REP and examines it to get a clear and meaningful picture of the database as quickly as possible. This so-called REP start index determines the current (start) index of this object extraction from the REPs. -1 lets the crawler take a random element from each REP, but if you want to do a full scan, you should start at 0 and observe the increment of this value to know where the crawler is at that moment. If this value exceeds the maximum length of a REP, the crawler is automatically stopped and you have thus scanned the entire REP. Under the runtime options you can limit the crawling duration of the object crawler. By default you let the crawler run until you stop it (the cancel button in the progressbar window closes the search with EndTXN and therefore the button is more a stop button than a cancel button) or until it is finished. You can stop the crawler automatically after the first run by selecting the option with the same name or only when the queue with all REP objects from the first run including their referenced objects is processed and empty by selecting the corresponding option Stop crawler when queue is empty. Depending on the selected option, you can control the behaviour in case of errors. In this sense, errors are not InstantView errors but logical consistency errors in the database. It is recommended to select the option Log error and continue. Under other settings you can define various options like: ObjectCrawler logging: all ObjectCrawler operations are written to separate log files. Process backreferences intelligently: if this option is selected, certain class checks are performed when examining backreferences to prevent errors. This is the description of the functionality of the module "metamodl" with the name "English:meta model" regarding ObjectCrawler: In this window the ObjectCrawler is given the parameters for traversing the database.